import jsonimport numpy as npTR_SIZE = 173109count = 0train_dict = []with open("../data/train_data.json", 'rb+') as train_data:    for lines in train_data:        count += 1        tr_data = json.loads(lines.decode())        train_dict.append(tr_data)        if count == TR_SIZE:            breaktr_text = []tr_label = []tr_spo = []# the_list = [[]]object_type = []subject_type = []the_object = []the_subject = []for i in range(count):    tr_text.append(train_dict[i]['text'])    tr_spo.append(train_dict[i]['spo_list'])    tr_label.append(tr_spo[i][0]['predicate'])    object_type.append((tr_spo[i][0]['object_type']))    the_object.append(tr_spo[i][0]['object'])    subject_type.append((tr_spo[i][0]['subject_type']))    the_subject.append(tr_spo[i][0]['subject'])# print(the_object)# print(object_type)def clearSen(comment):    comment = comment.strip(' ')    comment = comment.replace('、', ' ')    comment = comment.replace('~', ' ')    comment = comment.replace('～', ' ')    comment = comment.replace('…', ' ')    comment = comment.replace(']', ' ')    comment = comment.replace('[', ' ')    # comment = comment.replace('\f', ' ')    comment = comment.replace('】', ' ')    comment = comment.replace('、', ' ')    comment = comment.replace('【', ' ')    comment = comment.replace(' ', ' ')    comment = comment.replace('，', ' ')    comment = comment.replace('_', ' ')    comment = comment.replace('?', ' ')    comment = comment.replace('？', ' ')    comment = comment.replace(')', ' ')    comment = comment.replace('(', ' ')    comment = comment.replace('：', ' ')    comment = comment.replace('《', ' ')    comment = comment.replace('》', ' ')    comment = comment.replace('”', ' ')    comment = comment.replace('“', ' ')    comment = comment.replace('（', ' ')    comment = comment.replace('）', ' ')    comment = comment.replace(':', ' ')    comment = comment.replace('·', ' ')    comment = comment.replace('—', ' ')    comment = comment.replace('★', ' ')    return commentthe_text = []with open('./seg_spo.txt', 'r', encoding='utf-8') as ssf:#gb18030#seg_spo.txt文件中是训练的句子+三元组    for line in ssf:        line = clearSen(line)        # print(line)        line = line.strip('\n')        the_text.append(line)ssf.close()fenci_list = []  # 按空格切好句子for i in range(0, len(the_text)):    string = the_text[i].split()    fenci_list.append(string)# print(the_text[0])